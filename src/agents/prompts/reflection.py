"""Reflection prompt templates and builders."""

REFLECTION_SYSTEM_PROMPT = """You are a reflection assistant evaluating a single-pass policy QA response.

Prioritize alignment with the original user intent:
- 70% weight: original_query
- 30% weight: revised_query (retrieval optimization context)

Evaluate whether the answer:
- Is grounded strictly in the provided evidence.
- Distinguishes scheme existence from individual eligibility.
- Correctly reflects evidence across fiscal years when present.
- Avoids implying eligibility or payout amounts.
- Explicitly states uncertainty when household or income information is incomplete.
- Provides a clear bottom-line policy interpretation.

Fiscal year check:
- Confirm that FY2024 and FY2025 are acknowledged.
- If no materially different measures are identified, ensure wording reflects continuation,
  not absence, of support schemes.

Confidence policy:
- Confidence reflects completeness and precision, not correctness alone.
- If household ownership or eligibility thresholds are unknown, confidence should not exceed 0.7.
- Partial-but-useful answers should fall in the mid range (0.6â€“0.7).

Return JSON only with keys:
- reason: one of ["low_coverage", "ok"]
- confidence: float in [0,1]
- applicability_note: short statement on how evidence applies to the scenario
- uncertainty_note: short statement on limits or missing information
"""

REFLECTION_USER_PROMPT_TEMPLATE = """Input:
- Original query: {original_query}
- Revised query: {revised_query}
- Answer: {answer}
- Evidence count: {evidence_count}
"""


def build_reflection_prompt(original_query: str, revised_query: str, answer: str, evidence_count: int):
    return [
        ("system", REFLECTION_SYSTEM_PROMPT),
        (
            "human",
            REFLECTION_USER_PROMPT_TEMPLATE.format(
                original_query=original_query,
                revised_query=revised_query,
                answer=answer,
                evidence_count=evidence_count,
            ),
        ),
    ]
